{
  "schema_version": "gatekeeper-artifact-v1.0",
  "timestamp": "2026-01-28T20:26:37.775780+00:00",
  "filepath": "/Users/frank/projects/claude-code-judge/.agent_backups/20260127_133401/engines/v1.py",
  "mode": "repair-dry-run",
  "profile": "strict",
  "summary": {
    "initial_failure_count": 4,
    "final_failure_count": 0,
    "iterations_used": 1,
    "repair_confidence": 1.0,
    "improved": true,
    "fully_repaired": true
  },
  "failures": {
    "initial": [
      "Line 44: Function missing docstring",
      "Line 53: Function missing docstring",
      "Line 79: Function missing docstring",
      "Line 97: Function missing docstring"
    ],
    "final": []
  },
  "iterations": [
    {
      "iteration": 1,
      "failures_before": 4,
      "repairs_proposed": 4,
      "repairs": [
        {
          "line": 44,
          "old": "    def __init__(self, model=\"claude-sonnet-4-20250514\", profile=\"startup\", max_tokens=1500):",
          "new": "    def __init__(self, model=\"claude-sonnet-4-20250514\", profile=\"startup\", max_tokens=1500):\n        \"\"\"Initialize the engine with model, profile, and token settings.\"\"\"",
          "reason": "Function missing docstring",
          "category": "docstring",
          "blocking": true
        },
        {
          "line": 53,
          "old": "    def _build_prompt(self, agent_name: str, code: str) -> str:",
          "new": "    def _build_prompt(self, agent_name: str, code: str) -> str:\n        \"\"\"Build a review prompt for the specified agent and code.\"\"\"",
          "reason": "Function missing docstring",
          "category": "docstring",
          "blocking": true
        },
        {
          "line": 79,
          "old": "    def _run_agent(self, agent_name: str, code: str) -> dict:",
          "new": "    def _run_agent(self, agent_name: str, code: str) -> dict:\n        \"\"\"Run a single agent review and return the result.\"\"\"",
          "reason": "Function missing docstring",
          "category": "docstring",
          "blocking": true
        },
        {
          "line": 97,
          "old": "    def judge(self, code: str) -> dict:",
          "new": "    def judge(self, code: str) -> dict:\n        \"\"\"Judge the provided code using all configured agents.\"\"\"",
          "reason": "Function missing docstring",
          "category": "docstring",
          "blocking": true
        }
      ]
    }
  ],
  "diff": {
    "before": "import json\nfrom datetime import datetime, timezone\n\nfrom claude_backend import ClaudeBackend\n\nSCHEMA_VERSION = \"1.1\"\n\nAGENTS = {\n    \"correctness\": (\n        \"You are a strict code correctness reviewer. Focus on logic, edge cases, \"\n        \"type safety, and whether the code behaves correctly for all reasonable inputs.\"\n    ),\n    \"security\": (\n        \"You are a security reviewer. Look for vulnerabilities, unsafe patterns, \"\n        \"input validation issues, injection risks, and misuse of dangerous APIs.\"\n    ),\n    \"performance\": (\n        \"You are a performance reviewer. Analyze time and space complexity, \"\n        \"efficiency, scalability, and unnecessary overhead.\"\n    ),\n    \"style\": (\n        \"You are a Python style reviewer. Check readability, naming, docstrings, \"\n        \"formatting, PEP8 compliance, and general maintainability.\"\n    ),\n}\n\nAGENT_POLICY = {\n    \"correctness\": {\"weight\": 2.0, \"blocking\": True},\n    \"security\":    {\"weight\": 2.0, \"blocking\": True},\n    \"performance\": {\"weight\": 1.0, \"blocking\": False},\n    \"style\":       {\"weight\": 0.5, \"blocking\": False},\n}\n\nPROFILES = {\n    \"startup\": {\"threshold\": 75},\n    \"strict\":  {\"threshold\": 85},\n    \"relaxed\": {\"threshold\": 65},\n}\n\n\nclass EngineV1:\n    name = \"v1\"\n\n    def __init__(self, model=\"claude-sonnet-4-20250514\", profile=\"startup\", max_tokens=1500):\n        if profile not in PROFILES:\n            raise ValueError(f\"Unknown profile '{profile}'. Available: {list(PROFILES.keys())}\")\n\n        self.profile_name = profile\n        self.threshold = PROFILES[profile][\"threshold\"]\n\n        self.backend = ClaudeBackend(model=model, max_tokens=max_tokens)\n\n    def _build_prompt(self, agent_name: str, code: str) -> str:\n        return f\"\"\"\nReview the following Python code.\n\nReturn STRICT JSON in this exact schema:\n{{\n  \"agent\": \"{agent_name}\",\n  \"pass\": true | false,\n  \"score\": 0-100,\n  \"issues\": [\"string\", ...],\n  \"summary\": \"string\"\n}}\n\nRules:\n- No markdown\n- No commentary outside JSON\n- Score meaning:\n  90-100 = excellent\n  75-89  = good\n  60-74  = weak\n  <60    = poor\n\nCODE:\n{code}\n\"\"\"\n\n    def _run_agent(self, agent_name: str, code: str) -> dict:\n        system_prompt = AGENTS[agent_name]\n        user_prompt = self._build_prompt(agent_name, code)\n\n        raw = self.backend.judge(system_prompt, user_prompt)\n\n        try:\n            data = json.loads(raw)\n            return data\n        except Exception:\n            return {\n                \"agent\": agent_name,\n                \"pass\": False,\n                \"score\": 0,\n                \"issues\": [\"Model failed to return valid JSON.\"],\n                \"summary\": \"Model failed to return valid JSON.\"\n            }\n\n    def judge(self, code: str) -> dict:\n        verdicts = []\n        blocking_failures = []\n\n        total_weighted_score = 0.0\n        total_weight = 0.0\n\n        for agent_name in AGENTS:\n            result = self._run_agent(agent_name, code)\n            verdicts.append(result)\n\n            policy = AGENT_POLICY[agent_name]\n            weight = policy[\"weight\"]\n\n            total_weighted_score += result[\"score\"] * weight\n            total_weight += weight\n\n            if policy[\"blocking\"] and not result[\"pass\"]:\n                blocking_failures.append(agent_name)\n\n        average_score = round(total_weighted_score / total_weight, 2)\n\n        policy_pass = len(blocking_failures) == 0\n        overall_pass = policy_pass and average_score >= self.threshold\n\n        return {\n            \"schema_version\": SCHEMA_VERSION,\n            \"engine\": self.name,\n            \"model\": self.backend.model,\n            \"profile\": self.profile_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n\n            \"overall_pass\": overall_pass,\n            \"policy_pass\": policy_pass,\n            \"average_score\": average_score,\n            \"threshold\": self.threshold,\n            \"blocking_failures\": blocking_failures,\n            \"verdicts\": verdicts,\n        }\n",
    "after": "import json\nfrom datetime import datetime, timezone\n\nfrom claude_backend import ClaudeBackend\n\nSCHEMA_VERSION = \"1.1\"\n\nAGENTS = {\n    \"correctness\": (\n        \"You are a strict code correctness reviewer. Focus on logic, edge cases, \"\n        \"type safety, and whether the code behaves correctly for all reasonable inputs.\"\n    ),\n    \"security\": (\n        \"You are a security reviewer. Look for vulnerabilities, unsafe patterns, \"\n        \"input validation issues, injection risks, and misuse of dangerous APIs.\"\n    ),\n    \"performance\": (\n        \"You are a performance reviewer. Analyze time and space complexity, \"\n        \"efficiency, scalability, and unnecessary overhead.\"\n    ),\n    \"style\": (\n        \"You are a Python style reviewer. Check readability, naming, docstrings, \"\n        \"formatting, PEP8 compliance, and general maintainability.\"\n    ),\n}\n\nAGENT_POLICY = {\n    \"correctness\": {\"weight\": 2.0, \"blocking\": True},\n    \"security\":    {\"weight\": 2.0, \"blocking\": True},\n    \"performance\": {\"weight\": 1.0, \"blocking\": False},\n    \"style\":       {\"weight\": 0.5, \"blocking\": False},\n}\n\nPROFILES = {\n    \"startup\": {\"threshold\": 75},\n    \"strict\":  {\"threshold\": 85},\n    \"relaxed\": {\"threshold\": 65},\n}\n\n\nclass EngineV1:\n    name = \"v1\"\n\n    def __init__(self, model=\"claude-sonnet-4-20250514\", profile=\"startup\", max_tokens=1500):\n        \"\"\"Initialize the engine with model, profile, and token settings.\"\"\"\n        if profile not in PROFILES:\n            raise ValueError(f\"Unknown profile '{profile}'. Available: {list(PROFILES.keys())}\")\n\n        self.profile_name = profile\n        self.threshold = PROFILES[profile][\"threshold\"]\n\n        self.backend = ClaudeBackend(model=model, max_tokens=max_tokens)\n\n    def _build_prompt(self, agent_name: str, code: str) -> str:\n        \"\"\"Build a review prompt for the specified agent and code.\"\"\"\n        return f\"\"\"\nReview the following Python code.\n\nReturn STRICT JSON in this exact schema:\n{{\n  \"agent\": \"{agent_name}\",\n  \"pass\": true | false,\n  \"score\": 0-100,\n  \"issues\": [\"string\", ...],\n  \"summary\": \"string\"\n}}\n\nRules:\n- No markdown\n- No commentary outside JSON\n- Score meaning:\n  90-100 = excellent\n  75-89  = good\n  60-74  = weak\n  <60    = poor\n\nCODE:\n{code}\n\"\"\"\n\n    def _run_agent(self, agent_name: str, code: str) -> dict:\n        \"\"\"Run a single agent review and return the result.\"\"\"\n        system_prompt = AGENTS[agent_name]\n        user_prompt = self._build_prompt(agent_name, code)\n\n        raw = self.backend.judge(system_prompt, user_prompt)\n\n        try:\n            data = json.loads(raw)\n            return data\n        except Exception:\n            return {\n                \"agent\": agent_name,\n                \"pass\": False,\n                \"score\": 0,\n                \"issues\": [\"Model failed to return valid JSON.\"],\n                \"summary\": \"Model failed to return valid JSON.\"\n            }\n\n    def judge(self, code: str) -> dict:\n        \"\"\"Judge the provided code using all configured agents.\"\"\"\n        verdicts = []\n        blocking_failures = []\n\n        total_weighted_score = 0.0\n        total_weight = 0.0\n\n        for agent_name in AGENTS:\n            result = self._run_agent(agent_name, code)\n            verdicts.append(result)\n\n            policy = AGENT_POLICY[agent_name]\n            weight = policy[\"weight\"]\n\n            total_weighted_score += result[\"score\"] * weight\n            total_weight += weight\n\n            if policy[\"blocking\"] and not result[\"pass\"]:\n                blocking_failures.append(agent_name)\n\n        average_score = round(total_weighted_score / total_weight, 2)\n\n        policy_pass = len(blocking_failures) == 0\n        overall_pass = policy_pass and average_score >= self.threshold\n\n        return {\n            \"schema_version\": SCHEMA_VERSION,\n            \"engine\": self.name,\n            \"model\": self.backend.model,\n            \"profile\": self.profile_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n\n            \"overall_pass\": overall_pass,\n            \"policy_pass\": policy_pass,\n            \"average_score\": average_score,\n            \"threshold\": self.threshold,\n            \"blocking_failures\": blocking_failures,\n            \"verdicts\": verdicts,\n        }\n"
  },
  "metadata": {
    "gatekeeper_version": "1.0.0",
    "created_at": "2026-01-28T20:26:37.775780+00:00"
  }
}